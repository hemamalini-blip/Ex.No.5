

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS
REGISTER NO: 212222210006

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 

# Explanation: 
Define the Two Prompt Types:

Write a basic Prompt: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.
Based on that pattern type refined the prompt and submit that with AI tool.
Get the ouput and write the report.

Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a na√Øve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.
Run Experiments with ChatGPT:
Input the na√Øve prompt for each scenario and record the generated response.
Then input the corresponding basic prompt and capture that response.
Repeat this process for all selected scenarios to gather a full set of results.
Evaluate Responses : 
	Compare how ChatGPT performs when given na√Øve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where na√Øve prompts work equally well?
Deliverables:
A table comparing ChatGPT's responses to na√Øve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPT‚Äôs outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.


---

# üìë Report: Study of Prompt Templating Techniques for Automated Maintenance Report Generation

---

## **1. Introduction**

Prompt engineering is the process of designing inputs to guide AI models effectively. In this study, we explore **prompt templating techniques** for generating **automated maintenance reports**. The research compares two types of prompts‚Äî**Na√Øve Prompts** and **Basic Structured Prompts**‚Äîacross multiple test scenarios. The goal is to evaluate how **clarity and structure** in prompts influence the **quality, accuracy, and depth** of ChatGPT‚Äôs outputs.

---

## **2. Two Prompt Types Defined**

### **2.1 Na√Øve Prompt**

* Informal, vague, or incomplete instructions.
* Lacks structure or specific guidance.
* Example: *‚ÄúTell me about batteries.‚Äù*

### **2.2 Basic Structured Prompt**

* Clear, detailed, and organized instructions.
* Provides **context, constraints, and format expectations**.
* Example: *‚ÄúWrite a 150-word summary about lithium-ion batteries. Include working principle, advantages, disadvantages, and one industrial application.‚Äù*

---

## **3. Methodology**

* **Step 1**: Design na√Øve and basic prompts for each test scenario.
* **Step 2**: Run experiments with ChatGPT using both prompt types.
* **Step 3**: Record responses and compare on **quality, accuracy, and depth**.
* **Step 4**: Summarize insights into effective prompt structures.

---

## **4. Test Scenarios**

| **Scenario**              | **Na√Øve Prompt**            | **Basic Structured Prompt**                                                                                                         | **Na√Øve Response (shortened)**               | **Basic Response (shortened)**                      | **Evaluation**                                    |
| ------------------------- | --------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- | --------------------------------------------------- | ------------------------------------------------- |
| **Creative Story**        | ‚ÄúWrite a story.‚Äù            | ‚ÄúWrite a 200-word children‚Äôs story about a robot who learns kindness. Include beginning, conflict, and resolution.‚Äù                 | Very short, generic story without structure. | Full plot, moral lesson, and well-structured story. | Basic prompt much better (clarity + structure).   |
| **Factual Q\&A**          | ‚ÄúTell me about AI.‚Äù         | ‚ÄúExplain Artificial Intelligence in 120 words for an undergraduate student. Include definition, types, and one real-world example.‚Äù | Broad, mixed detail, lacks focus.            | Concise, structured, academic-friendly.             | Basic prompt more accurate and tailored.          |
| **Summarization**         | ‚ÄúSummarize climate change.‚Äù | ‚ÄúSummarize climate change in 100 words, covering causes, impacts, and solutions.‚Äù                                                   | Too general, missed impacts/solutions.       | Balanced coverage of all aspects.                   | Basic prompt ensured coverage of required points. |
| **Advice/Recommendation** | ‚ÄúGive me advice.‚Äù           | ‚ÄúProvide 5 practical tips for a college student to manage time effectively. Use bullet points.‚Äù                                     | Random advice, vague.                        | Clear, actionable, structured list.                 | Basic prompt ensures usable output.               |

---

## **5. Analysis of Results**

* **Quality**: Basic prompts consistently improved grammar, coherence, and readability.
* **Accuracy**: Structured prompts ensured correct inclusion of key points.
* **Depth**: Na√Øve prompts produced surface-level answers, while basic prompts encouraged detailed responses.
* **Consistency**: In all test scenarios, structured prompts gave more **reliable and professional outputs**.


---

## **6. Summary of Findings**

1. **Prompt clarity directly impacts AI performance**‚Äîstructured prompts deliver richer, more accurate outputs.
2. **Context and constraints** (like word limits, format, or target audience) guide the model to stay focused.
3. **Na√Øve prompts** are sufficient for trivial tasks but fail in complex or multi-step tasks.
4. **Best practice**: Always provide **role, context, task, and format** in a prompt template for optimal results.

---

## **7. Visuals**
<img width="1024" height="1024" alt="image" src="https://github.com/user-attachments/assets/ddb01e8b-6b4b-4522-8cb7-922700f0a9f3" />


### **7.1 Mind Map: Prompt Templating Techniques**

* Prompt Engineering

  * Types of Prompts

    * Na√Øve (vague, short, open-ended)
    * Basic Structured (detailed, contextual, clear)
  * Applications

    * Maintenance Reports
    * Summarization
    * Advice/Guidance
    * Creative Writing
  * Evaluation Metrics

    * Quality
    * Accuracy
    * Depth


   Here‚Äôs a conclusion you can directly use in your report:

---

## **8. Conclusion**

This study demonstrates that **prompt clarity and structure have a significant impact on AI-generated outputs**. Across all test scenarios, **basic structured prompts consistently outperformed na√Øve prompts** in terms of **quality, accuracy, and depth**. While na√Øve prompts produced vague and generic results, structured prompts guided the model to deliver well-organized, targeted, and reliable responses.

The findings confirm that **effective prompt templating is essential for automated report generation**, particularly in professional contexts such as maintenance documentation. By providing clear instructions, context, and output format, users can maximize the efficiency and usefulness of AI tools.

In conclusion, **structured prompt engineering is a key enabler for producing accurate, consistent, and actionable outputs**, making it an indispensable technique for automation in industries, education, and research.

---

Do you want me to also draft a **short executive summary** (1‚Äì2 paragraphs) so your report looks more professional?


---


---



# RESULT: The prompt for the above said problem executed successfully
